{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras訓練模型的方法  \n",
    "在之前，已經看過了使用`fit()`訓練模型的方法，假如資料龐大，無法將全部資料放進GPU RAM中，可以使用`tf.Data`與`Sequence 類`來解決。  \n",
    "在Keras中也有另外訓練model的方式`自定義`以及`train_on_batch`，`train_on_batch`幾乎與`fit`差不多，也是透過放入一個一個batch來做訓練。  \n",
    "在TF2.2以上還可用繼承`Model 類`修改當中的`train_step`來自訂訓練。  \n",
    "此次，是使用`自定義`來作範例。\n",
    "詳細資料查閱[Writing a training loop from scratch](https://keras.io/guides/writing_a_training_loop_from_scratch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "# 載入所需lib\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自訂義訓練需要使用`GradientTape`，在`GradientTape`中呼叫model，就可以根據loss與optimizer調整model.trainable的參數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "\n",
    "    # model layer\n",
    "    conv_1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu')\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling2D()\n",
    "    conv_2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling2D()\n",
    "    flatten = tf.keras.layers.Flatten()\n",
    "    drop = tf.keras.layers.Dropout(0.5)\n",
    "    output = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    # path\n",
    "    x = conv_1(inputs)\n",
    "    x = max_pool_1(x)\n",
    "    x = conv_2(x)\n",
    "    x = max_pool_2(x)\n",
    "    x = flatten(x)\n",
    "    x = drop(x)\n",
    "    x = output(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download MNIST dataset and preprocessing\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "y_train = y_train.astype('float32')\n",
    "x_test = x_test.astype('float32') / 255\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "y_train = np.expand_dims(y_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "y_test = np.expand_dims(y_test, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在`fit`中，有些參數需要設定，`batch_size`、`epochs`等等，自定義時候也需要進行處理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "# parameter init\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "batch_of_epoch = math.ceil(len(x_train) / batch_size)\n",
    "\n",
    "# optimizer\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "# loss\n",
    "loss_fu = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# accuracy\n",
    "acc = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始訓練循環"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of epoch 1\n",
      "Training accuracy on 1 epoch:\n",
      "loss: 0.2590, accuracy:90.0100%\n",
      "Use times:6.2576\n",
      "start of epoch 2\n",
      "Training accuracy on 2 epoch:\n",
      "loss: 0.1912, accuracy:96.7550%\n",
      "Use times:4.8390\n",
      "start of epoch 3\n",
      "Training accuracy on 3 epoch:\n",
      "loss: 0.1875, accuracy:97.4967%\n",
      "Use times:4.9229\n",
      "start of epoch 4\n",
      "Training accuracy on 4 epoch:\n",
      "loss: 0.1804, accuracy:97.9233%\n",
      "Use times:5.0508\n",
      "start of epoch 5\n",
      "Training accuracy on 5 epoch:\n",
      "loss: 0.1787, accuracy:98.1167%\n",
      "Use times:4.6721\n",
      "start of epoch 6\n",
      "Training accuracy on 6 epoch:\n",
      "loss: 0.1766, accuracy:98.3583%\n",
      "Use times:4.5598\n",
      "start of epoch 7\n",
      "Training accuracy on 7 epoch:\n",
      "loss: 0.1751, accuracy:98.4017%\n",
      "Use times:4.4798\n",
      "start of epoch 8\n",
      "Training accuracy on 8 epoch:\n",
      "loss: 0.1742, accuracy:98.6217%\n",
      "Use times:4.6399\n",
      "start of epoch 9\n",
      "Training accuracy on 9 epoch:\n",
      "loss: 0.1723, accuracy:98.6700%\n",
      "Use times:4.7882\n",
      "start of epoch 10\n",
      "Training accuracy on 10 epoch:\n",
      "loss: 0.1873, accuracy:98.7600%\n",
      "Use times:4.5140\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    #印出第幾個epoch\n",
    "    print('start of epoch {}'.format(epoch + 1))\n",
    "    \n",
    "    #每個batch進行訓練\n",
    "    for idx in range(batch_of_epoch):\n",
    "        x_batch = x_train[idx * batch_size : (idx + 1) * batch_size]\n",
    "        y_batch = y_train[idx * batch_size : (idx + 1) * batch_size]\n",
    "        \n",
    "        #使用GradientTape\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 某些layer，training與testing有所不同，所以訓練時代入training=True參數\n",
    "            pred = model(x_batch, training=True)\n",
    "            loss_value = loss_fu(y_batch, pred)\n",
    "        # 計算所有trainable weights對應的gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # 用optimizer調整trainable weights\n",
    "        opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        #更新 accuracy metric\n",
    "        acc.update_state(y_batch, pred)\n",
    "    \n",
    "    # 訓練完一個epoch，印出accuracy\n",
    "    print('Training accuracy on {} epoch:'.format(epoch + 1,))\n",
    "    print('loss:{:7.4f}, accuracy:{:7.4f}%'.format(loss_value, acc.result() * 100))\n",
    "    \n",
    "    # 清空accuracy state\n",
    "    acc.reset_states()\n",
    "    print('Use times:{:4.4f}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原本一個epoch只約兩秒鐘，但是自定義卻要4秒，這時候可以靠`tf.function`來提升性能。\n",
    "`tf.function`是將某些步驟轉換成static graph。  \n",
    "改寫一下training loop。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# clear session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = get_model()\n",
    "# 定義train function並且用tf.function裝飾\n",
    "@tf.function\n",
    "def train_data(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(x_batch, training=True)\n",
    "        loss_value = loss_fu(y_batch, pred)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    acc.update_state(y_batch, pred)\n",
    "    \n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of epoch 1\n",
      "Training accuracy on 1 epoch:\n",
      "loss: 1.4528, accuracy:98.9200%\n",
      "Use times:1.1737\n",
      "start of epoch 2\n",
      "Training accuracy on 2 epoch:\n",
      "loss: 0.3234, accuracy:99.9633%\n",
      "Use times:0.7426\n",
      "start of epoch 3\n",
      "Training accuracy on 3 epoch:\n",
      "loss: 0.2433, accuracy:99.9933%\n",
      "Use times:0.7224\n",
      "start of epoch 4\n",
      "Training accuracy on 4 epoch:\n",
      "loss: 0.1452, accuracy:99.9917%\n",
      "Use times:0.7287\n",
      "start of epoch 5\n",
      "Training accuracy on 5 epoch:\n",
      "loss: 0.0395, accuracy:99.9983%\n",
      "Use times:0.7254\n",
      "start of epoch 6\n",
      "Training accuracy on 6 epoch:\n",
      "loss: 0.0873, accuracy:99.9967%\n",
      "Use times:0.7274\n",
      "start of epoch 7\n",
      "Training accuracy on 7 epoch:\n",
      "loss: 0.1008, accuracy:99.9900%\n",
      "Use times:0.7199\n",
      "start of epoch 8\n",
      "Training accuracy on 8 epoch:\n",
      "loss: 0.0415, accuracy:99.9967%\n",
      "Use times:0.7356\n",
      "start of epoch 9\n",
      "Training accuracy on 9 epoch:\n",
      "loss: 0.0051, accuracy:99.9983%\n",
      "Use times:0.7471\n",
      "start of epoch 10\n",
      "Training accuracy on 10 epoch:\n",
      "loss: 0.0130, accuracy:99.9983%\n",
      "Use times:0.7226\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    #印出第幾個epoch\n",
    "    print('start of epoch {}'.format(epoch + 1))\n",
    "    \n",
    "    #每個batch進行訓練\n",
    "    for idx in range(batch_of_epoch):\n",
    "        x_batch = x_train[idx * batch_size : (idx + 1) * batch_size]\n",
    "        y_batch = y_train[idx * batch_size : (idx + 1) * batch_size]\n",
    "        \n",
    "        loss_value = train_data(x_batch, y_batch)\n",
    "    \n",
    "    # 訓練完一個epoch，印出accuracy\n",
    "    print('Training accuracy on {} epoch:'.format(epoch + 1,))\n",
    "    print('loss:{:7.4f}, accuracy:{:7.4f}%'.format(loss_value, acc.result() * 100))\n",
    "    \n",
    "    # 清空accuracy state\n",
    "    acc.reset_states()\n",
    "    print('Use times:{:4.4f}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到使用`tf.function`將一個epoch訓練時間縮小到不到1秒，但是如果是一些很複雜的操作，使用`tf.function`可能就沒有這麼好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **總結**  \n",
    "自定義訓練循環與`fit`大同小異，透過`tf.GradientTape`來算出gradient，再透過optimizer對model的權重進行更新。  \n",
    "在TF2.2之後，若想要有`fit`的一些優點，以及`自定義訓練`，可以改寫`Model類`的`train_step` method，預計在之後的文章說明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
