{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras的輔助工具-callbacks  \n",
    "在這之前已經了解了如何訓練一個model，但是有時候如果model已經無法收斂，或者想要調整訓練時的learning rate該如何呢？  \n",
    "這時候只要將**`callbacks`**參數加入`fit()`中就可以了。  \n",
    "Keras有一些內建好的callbacks可以使用，像是：\n",
    "+ ModelCheckpoint: 根據指定的評估方式來決定是否將model weights保留。  \n",
    "+ EarlyStopping: 根據指定的評估方式來決定是否停止訓練。  \n",
    "+ LearningRateScheduler: 可根據function定義learning rate，調整訓練時候的learning rate。  \n",
    "+ ReduceLROnPlateau: 根據指定的評估方式調整訓練時的learning rate。\n",
    "+ TensorBoard: 可以將訓練記錄成可視化的方法。  \n",
    "+ ...\n",
    "\n",
    "還有其他內建的callbacks，以上是比較常用的幾個，當然也可以自定義callbacks。  \n",
    "這次以EarlyStopping與ReduceLROnPlateau為例子。  \n",
    "更多callbacks訊息，請參閱[callbacks](https://keras.io/api/callbacks/)。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "# 載入所需lib\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "\n",
    "    # model layer\n",
    "    conv_1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu')\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling2D()\n",
    "    conv_2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling2D()\n",
    "    flatten = tf.keras.layers.Flatten()\n",
    "    drop = tf.keras.layers.Dropout(0.5)\n",
    "    output = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    # path\n",
    "    x = conv_1(inputs)\n",
    "    x = max_pool_1(x)\n",
    "    x = conv_2(x)\n",
    "    x = max_pool_2(x)\n",
    "    x = flatten(x)\n",
    "    x = drop(x)\n",
    "    x = output(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download MNIST dataset and preprocessing\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "y_train = y_train.astype('float32')\n",
    "x_test = x_test.astype('float32') / 255\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "y_train = np.expand_dims(y_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "y_test = np.expand_dims(y_test, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 用一個list存放選擇的callbacks\n",
    "# 衡量指標設定要與fit顯示的名稱相同\n",
    "\n",
    "'''\n",
    "early stopping 設定\n",
    "衡量指標為'val loss'\n",
    "衡量指標變化小於0.001，則判定沒有改變\n",
    "連續5次無改變則停止訓練\n",
    "停止訓練並且將最好的val loss時的weights寫入目前model\n",
    "發生更改的時候顯示訊息\n",
    "'''\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.001,\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1)\n",
    "\n",
    "'''\n",
    "ReduceLROnPlateau\n",
    "衡量指標為'val loss'\n",
    "降低learning rate的權重為0.1，new learning rate = learning * factor\n",
    "衡量指標變化小於0.001，則判定沒有改變\n",
    "連續2次無改變則降低learning rate\n",
    "learning rate最低降到1E-5\n",
    "發生更改的時候顯示訊息\n",
    "'''\n",
    "reLR = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_delta=0.001,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1)\n",
    "# 將所有設定好的callbacks放入list\n",
    "my_callbacks = [early_stop, reLR]\n",
    "\n",
    "# parameter init\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "# optimizer\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "# loss\n",
    "loss_fu = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# accuracy\n",
    "acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "model = get_model()\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將`my_callbacks`放入`fit()`中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 0.3584 - sparse_categorical_accuracy: 0.8930 - val_loss: 0.0842 - val_sparse_categorical_accuracy: 0.9767 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.1135 - sparse_categorical_accuracy: 0.9655 - val_loss: 0.0559 - val_sparse_categorical_accuracy: 0.9843 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0853 - sparse_categorical_accuracy: 0.9735 - val_loss: 0.0482 - val_sparse_categorical_accuracy: 0.9858 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0710 - sparse_categorical_accuracy: 0.9779 - val_loss: 0.0416 - val_sparse_categorical_accuracy: 0.9893 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0632 - sparse_categorical_accuracy: 0.9803 - val_loss: 0.0396 - val_sparse_categorical_accuracy: 0.9893 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0551 - sparse_categorical_accuracy: 0.9825 - val_loss: 0.0347 - val_sparse_categorical_accuracy: 0.9907 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0508 - sparse_categorical_accuracy: 0.9844 - val_loss: 0.0352 - val_sparse_categorical_accuracy: 0.9898 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "419/422 [============================>.] - ETA: 0s - loss: 0.0464 - sparse_categorical_accuracy: 0.9852\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0465 - sparse_categorical_accuracy: 0.9852 - val_loss: 0.0351 - val_sparse_categorical_accuracy: 0.9908 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0402 - sparse_categorical_accuracy: 0.9870 - val_loss: 0.0308 - val_sparse_categorical_accuracy: 0.9920 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0369 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0302 - val_sparse_categorical_accuracy: 0.9922 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "422/422 [==============================] - ETA: 0s - loss: 0.0365 - sparse_categorical_accuracy: 0.9884\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0365 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.0306 - val_sparse_categorical_accuracy: 0.9912 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0342 - sparse_categorical_accuracy: 0.9897 - val_loss: 0.0298 - val_sparse_categorical_accuracy: 0.9925 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0334 - sparse_categorical_accuracy: 0.9896 - val_loss: 0.0297 - val_sparse_categorical_accuracy: 0.9922 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "408/422 [============================>.] - ETA: 0s - loss: 0.0347 - sparse_categorical_accuracy: 0.9890\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0348 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.0297 - val_sparse_categorical_accuracy: 0.9920 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0355 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0295 - val_sparse_categorical_accuracy: 0.9920 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0342 - sparse_categorical_accuracy: 0.9891 - val_loss: 0.0295 - val_sparse_categorical_accuracy: 0.9920 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "421/422 [============================>.] - ETA: 0s - loss: 0.0355 - sparse_categorical_accuracy: 0.9889Restoring model weights from the end of the best epoch.\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0356 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0296 - val_sparse_categorical_accuracy: 0.9922 - lr: 1.0000e-05\n",
      "Epoch 00017: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    callbacks=my_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到原本設定要跑100個epoch，在地17個epoch時候就結束並且寫回最優weights，中間也因3次val loss判定連續沒有改變降低了learning rate，並且最後一次下降因為達到低標而設定在1E-5。  \n",
    "其他的`callbacks`用法也是相同概念，了解該`callbacks`用途以及設定好參數，再放入`fit()`中就可以使用，真的很方便。  \n",
    "當然，有些特殊需求也可以透過`自定義callbacks`來達成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **總結**  \n",
    "到此，使用Keras建立model的基本方法算是結束，只要將以下幾點完成即可生成一個基本的model。  \n",
    "+ 建立好模型的架構與定義好`input`與`output`\n",
    "+ 設定好相關參數與function(optimizer、loss function、callbacks等等)  \n",
    "+ 資料的前處理(ex:歸一化)與送入方式(ex:Sequence、tf.Data等等)  \n",
    "\n",
    "當然，最新的一些技術(ex:新的loss function)提供的API可能無法達成，這時候就得`自定義方式`處理。  \n",
    "接下來的主題將會環繞在`自定義`，也就是`Custom`的進階方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
