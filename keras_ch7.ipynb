{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras custom series-custom layer  \n",
    "一些比較新的技巧，可能內建的Functional API無法提供，這時候就必須自行實現。  \n",
    "在Keras中，可以使用`Layer class`自行定義運算方式，使用這個方式可以與其他Functional API一起串聯使用而不會有其他問題。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "costom layer有幾個method需要改寫：  \n",
    "+ \\_\\_init\\_\\_(self, ...):初始化參數。\n",
    "+ build(self, input_shape):從這邊設定初始化相關參數，可以從`input_shape`中得到input shape，自動產生相對應大小的weight。\n",
    "+ call(self, inputs):Tensor的路徑，中間運算在此完成。\n",
    "+ (opt)get_config(self):序列化用。\n",
    "+ (opt)from_config(cls, config):序列化用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "#載入所需lib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainable_weight & non-trainable_weight  \n",
    "在`__init__()`和`build()`中都可以建立weight。  \n",
    "weight又分為`trainable`與`non-trainable`，差別在使用optimizer時候weight是否進行調整。  \n",
    "以下是一個簡單的Dense layer創建方式：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(D_layer, self).__init__()\n",
    "        self.units = units\n",
    "    def build(self, input_shape):\n",
    "        #創建matrix weights\n",
    "        self.w = self.add_weight(\n",
    "            #matrix weights shape\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            #weight初始化方式\n",
    "            initializer='random_normal',\n",
    "            #設定是能夠修改\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    def get_config(self):\n",
    "        config = super(D_layer, self).get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試一下，假定輸入一個32-dim data有64-dim output，總trainable_weight應該是：  \n",
    "32*64(matrix) + 64(bias) = 2112。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "d_layer (D_layer)            (None, 64)                2112      \n",
      "=================================================================\n",
      "Total params: 2,112\n",
      "Trainable params: 2,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(32,))\n",
    "x = D_layer(64)(inputs)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add_loss()  \n",
    "通常希望model不要`overfitting`的話會使用`Regularization`方法限制weight大小，在custom layer中可以透過`add_loss()`將`regularization loss`加入。  \n",
    "若使用`fit()`訓練，則`regularization loss`會自行添加，若是自訂義可以呼叫`model.losses`或者`layer.losses`加入總loss後再進行調整weight。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_reg_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, rate=1e-3):\n",
    "        super(D_reg_layer, self).__init__()\n",
    "        self.units = units\n",
    "        self.rate = rate\n",
    "    def build(self, input_shape):\n",
    "        #創建matrix weights\n",
    "        self.w = self.add_weight(\n",
    "            #matrix weights shape\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            #weight初始化方式\n",
    "            initializer='random_normal',\n",
    "            #設定是能夠修改\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "    def call(self, inputs):\n",
    "        #add L2-reularization loss\n",
    "        self.add_loss(self.rate * tf.math.reduce_sum(tf.math.square(self.w)))\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    def get_config(self):\n",
    "        config = super(D_reg_layer, self).get_config()\n",
    "        config.update({'units': self.units, 'rate': self.rate})\n",
    "        return config\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "d_reg_layer (D_reg_layer)    (None, 64)                2112      \n",
      "=================================================================\n",
      "Total params: 2,112\n",
      "Trainable params: 2,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "inputs = tf.keras.Input(shape=(32,))\n",
    "x = D_reg_layer(64)(inputs)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "model.summary()\n",
    "_ = model(tf.zeros((1, 32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看regularization loss："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0051943753"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(model.losses).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer的losses："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005062524"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_layer = D_reg_layer(64)\n",
    "test_layer(tf.zeros((1, 32)))\n",
    "sum(test_layer.losses).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training 與 inference不同調的layer  \n",
    "`BatchNormalization`與`Dropout系列`，training與inference是有所不同的，Dropout在training時候，會隨機丟棄node，但是在inference時候，會將所有output保留並且乘上一個權重(在實際運作上，在training時會將輸出除以保留機率，在inference時就不除保留)。  \n",
    "若custom layer也有相似的運行模式，則在`call()` method中加入`training=None`，透過這個參數就可以控制是在training phase還是inference phase。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1.4285715 0.        1.4285715 1.4285715 1.4285715]], shape=(1, 5), dtype=float32)\n",
      "tf.Tensor([[1. 1. 1. 1. 1.]], shape=(1, 5), dtype=float32)\n",
      "[[1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "inputs = tf.keras.Input(shape=(5,))\n",
    "x = tf.keras.layers.Dropout(0.3)(inputs)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "print(model(tf.ones((1, 5)), training=True))\n",
    "print(model(tf.ones((1, 5))))\n",
    "print(model.predict(tf.ones((1, 5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由這可知道，當使用`model(x)`與`model.predict(x)`，皆為inference phase，若要觀察training phase則須帶入`training=True`參數，這個在自定義training中若有這種不同步的layer記得加入。  \n",
    "接著，用DropBlock來演示自定義layer如何實現不同步的layer。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DropBlock - 用於圖像的Dropout  \n",
    "[DropBlock](https://arxiv.org/abs/1810.12890)：根據論文內容，比傳統Dropout更能提升圖像分類的準確度。  \n",
    "想法也很簡單，圖像是有地域性的，所以一次丟掉一塊而不是一個像素更能找出更好的特徵。  \n",
    "以下實現採用與TensorFlow Dropout相同思路。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropBlock(tf.keras.layers.Layer):\n",
    "    # drop機率、block size\n",
    "    def __init__(self, drop_rate=0.3, block_size=3, **kwargs):\n",
    "        super(DropBlock, self).__init__(**kwargs)\n",
    "        self.rate = drop_rate\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    # 加入training parameter，用以判斷training phase or inference phase\n",
    "    def call(self, inputs, training=None):\n",
    "        # training phase\n",
    "        if training:\n",
    "            b = tf.shape(inputs)[0]\n",
    "            random_tensor = tf.random.uniform(shape=[b, self.m_h, self.m_w, self.c]) + self.bernoulli_rate\n",
    "            binary_tensor = tf.floor(random_tensor)\n",
    "            binary_tensor = tf.pad(\n",
    "                binary_tensor,\n",
    "                [[0, 0], [self.block_size // 2, self.block_size // 2], [self.block_size // 2, self.block_size // 2], [0, 0]]\n",
    "            )\n",
    "            binary_tensor = tf.nn.max_pool(\n",
    "                binary_tensor,\n",
    "                [1, self.block_size, self.block_size, 1],\n",
    "                [1,1,1,1],\n",
    "                'SAME'\n",
    "            )\n",
    "            binary_tensor = 1 - binary_tensor\n",
    "            inputs = tf.math.divide(inputs, (1 - self.rate)) * binary_tensor\n",
    "        return inputs\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.b, self.h, self.w, self.c = input_shape.as_list()\n",
    "        self.m_h = self.h - (self.block_size // 2) * 2\n",
    "        self.m_w = self.w - (self.block_size // 2) * 2\n",
    "        self.bernoulli_rate = (self.rate * self.h * self.w) / (self.m_h * self.m_w * self.block_size**2)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(DropBlock, self).get_config()\n",
    "        config.update({'rate': self.rate, 'block_size': self.block_size})\n",
    "        return config\n",
    "    \n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model(x, training=True):\n",
      "[[[0.        0.        0.        1.4285715 1.4285715]\n",
      "  [0.        0.        0.        1.4285715 1.4285715]\n",
      "  [0.        0.        0.        1.4285715 1.4285715]\n",
      "  [1.4285715 1.4285715 1.4285715 1.4285715 1.4285715]\n",
      "  [1.4285715 1.4285715 1.4285715 1.4285715 1.4285715]]]\n",
      "[[[0.        0.        0.        1.4285715 1.4285715]\n",
      "  [0.        0.        0.        1.4285715 1.4285715]\n",
      "  [0.        0.        0.        1.4285715 1.4285715]\n",
      "  [1.4285715 1.4285715 1.4285715 1.4285715 1.4285715]\n",
      "  [1.4285715 1.4285715 1.4285715 1.4285715 1.4285715]]]\n",
      "[[[0.        0.        0.        1.4285715 1.4285715]\n",
      "  [0.        0.        0.        1.4285715 1.4285715]\n",
      "  [0.        0.        0.        1.4285715 1.4285715]\n",
      "  [1.4285715 1.4285715 1.4285715 1.4285715 1.4285715]\n",
      "  [1.4285715 1.4285715 1.4285715 1.4285715 1.4285715]]]\n",
      "--------------------------------------------------\n",
      "model(x):\n",
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]]\n",
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]]\n",
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]]\n",
      "--------------------------------------------------\n",
      "model.predict(x):\n",
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]]\n",
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]]\n",
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "inputs = tf.keras.Input(shape=(5, 5, 3))\n",
    "x = DropBlock()(inputs)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "result = model(tf.ones((1,5,5,3)), training=True)\n",
    "print('model(x, training=True):')\n",
    "for i in range(3):\n",
    "    print(result[:,:,:,1].numpy())\n",
    "print('-'*50)\n",
    "result = model(tf.ones((1,5,5,3)))\n",
    "print('model(x):')\n",
    "for i in range(3):\n",
    "    print(result[:,:,:,1].numpy())\n",
    "print('-'*50)\n",
    "print('model.predict(x):')\n",
    "result = model.predict(tf.ones((1,5,5,3)))\n",
    "for i in range(3):\n",
    "    print(result[:,:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layer 可以包含其他layer  \n",
    "自定義layer也可以包含其他layer做運算，包含custom layer、functional API。  \n",
    "使用custom layer將之前的例子包裝。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(tf.keras.layers.Layer):\n",
    "    def __init__(self,name=None, **kwargs):\n",
    "        super(MNIST, self).__init__(name=name, **kwargs)\n",
    "        self.conv_1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu')\n",
    "        self.max_pool_1 = tf.keras.layers.MaxPooling2D()\n",
    "        self.conv_2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')\n",
    "        self.max_pool_2 = tf.keras.layers.MaxPooling2D()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.drop = tf.keras.layers.Dropout(0.5)\n",
    "        self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.max_pool_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.max_pool_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "mnist (MNIST)                (None, 10)                34826     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "x = MNIST()(inputs)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255\n",
    "y_train = y_train.astype('float32')\n",
    "x_test = x_test.astype('float32') / 255\n",
    "y_test = y_test.astype('float32')\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "y_train = np.expand_dims(y_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "y_test = np.expand_dims(y_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 0.3788 - sparse_categorical_accuracy: 0.8846 - val_loss: 0.0844 - val_sparse_categorical_accuracy: 0.9777\n",
      "Epoch 2/2\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.1133 - sparse_categorical_accuracy: 0.9659 - val_loss: 0.0560 - val_sparse_categorical_accuracy: 0.9847\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **總結**  \n",
    "Custom layer可以很方便的自訂義tensor運算，對於實現一些新的論文方法有很高的靈活度，並且照著規範處理，其餘的部分就不須花時間處理。  \n",
    "主要三個method：  \n",
    "+ `__init__()`:初始化的參數。  \n",
    "+ `build()`:根據前一個layer的output shape進行本層的動態參數設定。  \n",
    "+ `call()`: tensor向前運算的區域，對於training與inference不同模式的layer，記得增加`training=None`參數提供判斷。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
